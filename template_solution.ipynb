{"cells":[{"cell_type":"markdown","id":"57002395eb6b50f8","metadata":{"collapsed":false,"id":"57002395eb6b50f8"},"source":["# Task 4\n","This serves as a template which will guide you through the implementation of this task. It is advised to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps.\n","This is the jupyter notebook version of the template. For the python file version, please refer to the file `template_solution.py`."]},{"cell_type":"markdown","id":"c0a46aca0e5d9ef","metadata":{"collapsed":false,"id":"c0a46aca0e5d9ef"},"source":["First, we import necessary libraries:"]},{"cell_type":"code","execution_count":null,"id":"b1c6ff7632991155","metadata":{"id":"b1c6ff7632991155"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","# Add any other imports you need here\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import StepLR\n","from torch.cuda.amp import GradScaler, autocast"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-KwdPMpEzZr1","executionInfo":{"status":"ok","timestamp":1715671072956,"user_tz":-120,"elapsed":20826,"user":{"displayName":"Léopold Das","userId":"00868182417539780238"}},"outputId":"bf159774-2c3a-420d-8998-1381d0f95101"},"id":"-KwdPMpEzZr1","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"6b5cba49","metadata":{"id":"6b5cba49"},"source":["Depending on your approach, you might need to adapt the structure of this template or parts not marked by TODOs.\n","It is not necessary to completely follow this template. Feel free to add more code and delete any parts that are not required."]},{"cell_type":"code","execution_count":null,"id":"8d9b5e89d4b3a02","metadata":{"id":"8d9b5e89d4b3a02"},"outputs":[],"source":["DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","BATCH_SIZE = 16  # TODO: Set the batch size according to both training performance and available memory\n","NUM_EPOCHS = 15  # TODO: Set the number of epochs\n","\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","train_val = pd.read_csv(\"/content/drive/MyDrive/Machine_learning/task4_hr35z9/train.csv\")\n","test_val = pd.read_csv(\"/content/drive/MyDrive/Machine_learning/task4_hr35z9/test_no_score.csv\")"]},{"cell_type":"code","source":["text = \"Hello, how are you?\"\n","encoded_input = tokenizer(text, truncation=True, padding='max_length', max_length=512)\n","print(encoded_input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQYDTYvb2DqB","executionInfo":{"status":"ok","timestamp":1715676205689,"user_tz":-120,"elapsed":380,"user":{"displayName":"Léopold Das","userId":"00868182417539780238"}},"outputId":"44c8abd0-5e47-4688-9a56-6b3992ba9d1b"},"id":"PQYDTYvb2DqB","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}]},{"cell_type":"code","execution_count":null,"id":"161fdafaedaa5b0","metadata":{"id":"161fdafaedaa5b0"},"outputs":[],"source":["# TODO: Fill out ReviewDataset\n","class ReviewDataset(Dataset):\n","    def __init__(self, data_frame, train = True):\n","        self.data_frame = data_frame\n","        self.train = train\n","\n","    def __len__(self):\n","        return len(self.data_frame)\n","\n","    def __getitem__(self, index):\n","        text = self.data_frame.iloc[index]['title'] + \" \" + self.data_frame.iloc[index]['sentence']\n","\n","        inputs = tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n","\n","        input_ids = inputs['input_ids'].squeeze(0)  # Remove batch dimension\n","        attention_mask = inputs['attention_mask'].squeeze(0)\n","\n","        if self.train:\n","            label = self.data_frame.iloc[index]['score']\n","            return input_ids, attention_mask, label\n","\n","        return input_ids, attention_mask"]},{"cell_type":"code","execution_count":null,"id":"676f7a012f50e988","metadata":{"id":"676f7a012f50e988"},"outputs":[],"source":["train_dataset = ReviewDataset(train_val, train = True)\n","test_dataset = ReviewDataset(test_val, train = False)\n","\n","train_loader = DataLoader(dataset=train_dataset,\n","                          batch_size=BATCH_SIZE,\n","                          shuffle=True, num_workers=4, pin_memory=True)\n","test_loader = DataLoader(dataset=test_dataset,\n","                         batch_size=BATCH_SIZE,\n","                         shuffle=False, num_workers=4, pin_memory=True)\n","# Additional code if needed"]},{"cell_type":"code","execution_count":null,"id":"843f38b9dbea00b0","metadata":{"id":"843f38b9dbea00b0"},"outputs":[],"source":["# TODO: Fill out MyModule\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super(MyModule, self).__init__()\n","        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","        #config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_attention_heads=8, dim=512)  # Reduced dimensionality\n","        self.pre_classifier = nn.Linear(768, 768)  # Example additional layer\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(768, 1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = distilbert_output[0]\n","        pooled_output = hidden_state[:, 0]\n","        x = self.pre_classifier(pooled_output)\n","        x = nn.ReLU()(x)\n","        x = self.dropout(x)\n","        output = self.classifier(x)\n","        return 10 * torch.sigmoid(output)\n","\n","model = MyModule().to(DEVICE)"]},{"cell_type":"code","execution_count":null,"id":"605e5bd0373a1dda","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"605e5bd0373a1dda","executionInfo":{"status":"ok","timestamp":1715680068788,"user_tz":-120,"elapsed":1726785,"user":{"displayName":"Léopold Das","userId":"00868182417539780238"}},"outputId":"08a3add7-7306-4727-c87c-392c2ef1a502"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/782 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","100%|██████████| 782/782 [03:36<00:00,  3.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/8, Loss: 0.397436794193695\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 782/782 [03:36<00:00,  3.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/8, Loss: 0.3087817595180724\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 782/782 [03:35<00:00,  3.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/8, Loss: 0.3108519027652719\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 782/782 [03:35<00:00,  3.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/8, Loss: 0.41446485233200175\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 782/782 [03:35<00:00,  3.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/8, Loss: 0.21451863356153755\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 782/782 [03:35<00:00,  3.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/8, Loss: 0.1472221712946244\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 782/782 [03:35<00:00,  3.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/8, Loss: 0.10414763433558633\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 782/782 [03:35<00:00,  3.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/8, Loss: 0.0921176433029687\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# TODO: Setup loss function, optimiser, and scheduler\n","criterion = nn.MSELoss()\n","optimiser = torch.optim.Adam(model.parameters(), lr=5e-5)\n","scheduler = StepLR(optimiser, step_size=10, gamma=0.1)\n","\n","model.train()\n","for epoch in range(NUM_EPOCHS):\n","    running_loss = 0.0\n","    scaler = GradScaler()\n","    for batch in tqdm(train_loader, total=len(train_loader)):\n","        input_ids, attention_mask, labels = batch\n","\n","        input_ids = input_ids.to(DEVICE)\n","        attention_mask = attention_mask.to(DEVICE).float()\n","        labels = labels.to(DEVICE).float()\n","        optimiser.zero_grad()\n","\n","        with autocast():  # Run model in mixed precision\n","            outputs = model(input_ids, attention_mask)\n","            outputs = outputs.float()\n","            loss = criterion(outputs, labels.unsqueeze(1))\n","\n","        scaler.scale(loss).backward()  # Scale loss to adjust for reduced precision\n","        scaler.step(optimiser)\n","        scaler.update()\n","\n","        running_loss += loss.item()\n","\n","    # Step the scheduler\n","    scheduler.step()\n","\n","    # Print average loss for the epoch\n","    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {running_loss / len(train_loader)}')"]},{"cell_type":"code","source":[],"metadata":{"id":"Cyn3L1yyKzrG"},"id":"Cyn3L1yyKzrG","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"60e56bba5fa2d905","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60e56bba5fa2d905","executionInfo":{"status":"ok","timestamp":1715680099555,"user_tz":-120,"elapsed":17318,"user":{"displayName":"Léopold Das","userId":"00868182417539780238"}},"outputId":"77c4b017-aa68-4549-fd69-9cebf93e5b03"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/63 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","100%|██████████| 63/63 [00:16<00:00,  4.33it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","100%|██████████| 63/63 [00:17<00:00,  3.69it/s]\n"]}],"source":["model.eval()\n","with torch.no_grad():\n","    results = []\n","    for batch in tqdm(test_loader, total=len(test_loader)):\n","        #batch = batch.to(DEVICE)\n","\n","        # TODO: Set up evaluation loop\n","\n","        input_ids, attention_mask = batch\n","        input_ids = input_ids.to(DEVICE)\n","        attention_mask = attention_mask.to(DEVICE).float()\n","        outputs = model(input_ids, attention_mask)\n","        outputs = outputs.squeeze().cpu().numpy()\n","        results.append(outputs)\n","\n","\n","\n","    with open(\"/content/drive/MyDrive/Machine_learning/task4_hr35z9/result.txt\", \"w\") as f:\n","        for val in np.concatenate(results):\n","            f.write(f\"{val}\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}